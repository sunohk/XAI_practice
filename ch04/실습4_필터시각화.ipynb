{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'train_img' = p'train-images-idx3-ubyte.gz'\n",
    "'train_label':'train-labels-idx1-ubyte.gz'\n",
    "'test_img':'t10k-images-idx3-ubyte.gz'\n",
    "'test_label':'t10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 13:24:33.886244: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/sunohk/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101a40a9cc75427e8b6f4b0e7b081e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/sunohk/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 13:24:37.596070: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-09-29 13:24:37.596100: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-09-29 13:24:37.596112: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-09-29 13:24:37.596193: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-29 13:24:37.596244: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data, mnist_info = tfds.load('mnist', with_info=True, as_supervised=True)\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/Users/sunohk/tensorflow_datasets/mnist/3.0.1.incompleteNAONOP',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalize to [0, 1] #픽셀값이 255이내 이므로\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0    # Normalize to [0, 1]\n",
    "y_train = to_categorical(y_train, num_classes=10)  # One-hot encoding\n",
    "y_test = to_categorical(y_test, num_classes=10)      # One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x-in (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 5)         130       \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 14, 14, 5)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 14, 14, 5)         630       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 7, 7, 5)           0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 7, 7, 20)          2520      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 3, 3, 20)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 180)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5090 (19.88 KB)\n",
      "Trainable params: 5090 (19.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1), name='x-in')\n",
    "\n",
    "# Conv2D layer 1 \n",
    "    #ReLu를 사용한 이유 : \n",
    "    # 1) 합성곱 층에서는 입력 데이터의 특징을 추출하는 것이 중요 -> ReLu는 비선형성을 제공하여 신경망이 더 복잡한 패턴을 학습할 수 있도록 함\n",
    "    # 2) 양수 입력에 대한 기울기가 1이므로 학습 시 기울기 소실 문제(역전파 과정에서 출력층에서 멀어질수록 = 입력층과 가까울수록 기울기 값이 매우 작아지는 현상)를 줄여줌\n",
    "    # 3) 계산 효율성 = ReLu는 계산이 간단하고 빠름\n",
    "    # 4) 음수 값을 0으로 만들기 때문에 중요하지 않은 정보는 무시, 중요한 특징에 집중하게 함\n",
    "conv1 = layers.Conv2D(5, (5, 5), activation='relu', padding='same')(inputs) #28*28\n",
    "pool1 = layers.MaxPooling2D((2, 2))(conv1) #maxpooling 을 통해 2*2 크기로 축소\n",
    "\n",
    "# Conv2D layer 2\n",
    "conv2 = layers.Conv2D(5, (5, 5), activation='relu', padding='same')(pool1) #14*14\n",
    "pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "# Conv2D layer 3\n",
    "conv3 = layers.Conv2D(20, (5, 5), activation='relu', padding='same')(pool2) #7*7\n",
    "pool3 = layers.MaxPooling2D((2, 2))(conv3) #3*3 ( 7/2=3.*** 이므로 )\n",
    "\n",
    "# Flatten layer\n",
    "flat = layers.Flatten()(pool3) #1차원 배열로 변환 -> fully conncted layer로 연결할 수 있음 3*3*5 = 45 크기\n",
    "\n",
    "# Fully connected layer with softmax activation \n",
    "outputs = layers.Dense(10, activation='softmax')(flat) \n",
    "    #dense : 완전연결층 , 10 = 0~9까지의 수를 분류하는 10개의 클래스\n",
    "    #softmax : 주어진 입력 벡터를 확률 분포로 변환(0~1 사이 값으로 출력, 모든 출력 값의 합은 1) = 다중 클래스 분류 문제에서 각 클래스에 대한 확률 제공\n",
    "\n",
    "# Create the model\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Use legacy Adam optimizer to avoid M1/M2 slowdown\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(1e-4) # adam 최적화기(optimizer) 사용\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy', #다중 클래스(클래스 3개 이상) 분류 문제에서 사용되는 손실 함수(cross entropy : 예측과 실제의 확률 분포 차이 측정)\n",
    "              metrics=['accuracy']) # 평가 지표로 정확도 사용\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.0875 - val_accuracy: 0.9840\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0411 - accuracy: 0.9903 - val_loss: 0.0421 - val_accuracy: 0.9863\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0434 - accuracy: 0.9901 - val_loss: 0.0457 - val_accuracy: 0.9867\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0375 - accuracy: 0.9905 - val_loss: 0.0445 - val_accuracy: 0.9869\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0410 - accuracy: 0.9905 - val_loss: 0.0414 - val_accuracy: 0.9865\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0446 - accuracy: 0.9899 - val_loss: 0.0461 - val_accuracy: 0.9867\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.0412 - val_accuracy: 0.9874\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0339 - accuracy: 0.9911 - val_loss: 0.0403 - val_accuracy: 0.9873\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0376 - accuracy: 0.9909 - val_loss: 0.0608 - val_accuracy: 0.9826\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0372 - accuracy: 0.9906 - val_loss: 0.0395 - val_accuracy: 0.9872\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0312 - accuracy: 0.9912 - val_loss: 0.0393 - val_accuracy: 0.9872\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0338 - accuracy: 0.9913 - val_loss: 0.0419 - val_accuracy: 0.9865\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 11s 9ms/step - loss: 0.0370 - accuracy: 0.9911 - val_loss: 0.0395 - val_accuracy: 0.9874\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 11s 9ms/step - loss: 0.0286 - accuracy: 0.9916 - val_loss: 0.0383 - val_accuracy: 0.9879\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 11s 9ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 0.0380 - val_accuracy: 0.9887\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 11s 9ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.0439 - val_accuracy: 0.9874\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0338 - accuracy: 0.9918 - val_loss: 0.0442 - val_accuracy: 0.9875\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.0334 - accuracy: 0.9921 - val_loss: 0.0489 - val_accuracy: 0.9870\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0370 - accuracy: 0.9918 - val_loss: 0.0453 - val_accuracy: 0.9870\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0384 - accuracy: 0.9915 - val_loss: 0.0418 - val_accuracy: 0.9876\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0349 - accuracy: 0.9917 - val_loss: 0.0518 - val_accuracy: 0.9860\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0460 - accuracy: 0.9908 - val_loss: 0.0493 - val_accuracy: 0.9864\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0389 - accuracy: 0.9910 - val_loss: 0.0419 - val_accuracy: 0.9874\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0354 - accuracy: 0.9914 - val_loss: 0.0457 - val_accuracy: 0.9866\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.0438 - val_accuracy: 0.9869\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0385 - accuracy: 0.9915 - val_loss: 0.0402 - val_accuracy: 0.9878\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.0413 - val_accuracy: 0.9877\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0396 - accuracy: 0.9913 - val_loss: 0.0538 - val_accuracy: 0.9870\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0417 - accuracy: 0.9912 - val_loss: 0.0459 - val_accuracy: 0.9876\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0322 - accuracy: 0.9921 - val_loss: 0.0483 - val_accuracy: 0.9863\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0325 - accuracy: 0.9921 - val_loss: 0.0410 - val_accuracy: 0.9879\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0341 - accuracy: 0.9916 - val_loss: 0.0438 - val_accuracy: 0.9874\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0374 - accuracy: 0.9916 - val_loss: 0.0441 - val_accuracy: 0.9869\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0329 - accuracy: 0.9920 - val_loss: 0.0389 - val_accuracy: 0.9882\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.0301 - accuracy: 0.9923 - val_loss: 0.0398 - val_accuracy: 0.9875\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.0295 - accuracy: 0.9921 - val_loss: 0.0498 - val_accuracy: 0.9864\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0372 - accuracy: 0.9917 - val_loss: 0.0394 - val_accuracy: 0.9873\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.0277 - accuracy: 0.9926 - val_loss: 0.0387 - val_accuracy: 0.9877\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0354 - accuracy: 0.9918 - val_loss: 0.0546 - val_accuracy: 0.9861\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0382 - accuracy: 0.9918 - val_loss: 0.0630 - val_accuracy: 0.9858\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0308 - accuracy: 0.9924 - val_loss: 0.0548 - val_accuracy: 0.9862\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0322 - accuracy: 0.9926 - val_loss: 0.0395 - val_accuracy: 0.9875\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0301 - accuracy: 0.9925 - val_loss: 0.0612 - val_accuracy: 0.9865\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0339 - accuracy: 0.9926 - val_loss: 0.0567 - val_accuracy: 0.9867\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0307 - accuracy: 0.9926 - val_loss: 0.0386 - val_accuracy: 0.9879\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0261 - accuracy: 0.9928 - val_loss: 0.0382 - val_accuracy: 0.9884\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 0.0478 - val_accuracy: 0.9873\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0249 - accuracy: 0.9930 - val_loss: 0.0374 - val_accuracy: 0.9883\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0315 - accuracy: 0.9927 - val_loss: 0.0384 - val_accuracy: 0.9884\n",
      "Epoch 50/50\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.0272 - accuracy: 0.9929 - val_loss: 0.0376 - val_accuracy: 0.9881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 14:58:12.859287: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9937\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = model.evaluate(x_train, y_train, verbose=0)[1] # 손실값, 정확도 => 중에서 정확도를 출력하기 위해 [1] 인덱싱\n",
    "print(\"Training accuracy: %.4f\" % train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.9881\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(\"Test accuracy : %.4f\" % test_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as an HDF5 file\n",
    "model.save('./mnist_model.h5')  # HDF5 format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
